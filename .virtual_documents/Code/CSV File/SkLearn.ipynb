


import plotly.io as pio
pio.renderers.default = "notebook+pdf"


import pandas as pd
import numpy as np

import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go

from sklearn.impute import SimpleImputer
from sklearn.model_selection import StratifiedShuffleSplit
from typing import List
from sklearn.preprocessing import RobustScaler,StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import precision_recall_fscore_support

from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.over_sampling import RandomOverSampler

from plotly.offline import plot, iplot, init_notebook_mode
init_notebook_mode(connected=True)

import warnings

warnings.filterwarnings('ignore')


df=pd.read_csv("./Data/kag_risk_factors_cervical_cancer.csv")
df.head()


target = 'Biopsy'


df=df.drop_duplicates()
df.info()


df = df.replace('?', np.nan)

print("Check all Nan counts")
df.isnull().sum()


df = df.apply(pd.to_numeric, errors='coerce')
df = df.fillna(df.median())

print("Check all Nan counts")
df.isnull().sum()





import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from imblearn.over_sampling import ADASYN
import time


def training_loop(df, target, performance_metrics):
    # Split data into training and testing dataset
    X = df.drop(columns=[target])
    y = df[target].copy()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, shuffle=True)

    # Apply ADASYN only on the training data
    adasyn = ADASYN()
    X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)

    # Model list using Scikit-learn classifiers
    models = [
        ("Logistic Regression", LogisticRegression(max_iter=1000)),
        ("Decision Tree", DecisionTreeClassifier()),
        ("Random Forest", RandomForestClassifier()),
        ("Gradient Boosting", GradientBoostingClassifier()),
        ("SVC", SVC(probability=True)),
        ("Naive Bayes", GaussianNB())
    ]

    results = []

    for name, model in models:
        start_time = time.time()
        model.fit(X_train_resampled, y_train_resampled)
        predictions = model.predict(X_test)
        elapsed_time = time.time() - start_time

        # Evaluate performance
        accuracy = accuracy_score(y_test, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions, average='binary')

        results.append({
            'Name': name,
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1,
            'Time(s)': elapsed_time
        })

    return pd.DataFrame(results)

# Example of using the function with a DataFrame `df` and a target column `target`
performance_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Time(s)']
performance_df = pd.DataFrame()

# Assuming `df` is your DataFrame and 'target' is your target column
# Repeat the experiment multiple times as needed
for _ in range(10):
    result = training_loop(df, target, performance_metrics)
    performance_df = pd.concat([performance_df, result], ignore_index=True)



avg_df=performance_df.groupby('Name',as_index=False)[performance_metrics].mean()
acc_comparison = px.bar(avg_df, x="Name",
                        y=performance_metrics,
                        barmode="group")
acc_comparison.show()


px.bar(performance_df.groupby('Name',as_index=False)['Time(s)'].mean(),x="Name",y=['Time(s)'],color='Name')



performance_df.groupby('Name',as_index=False).mean()




